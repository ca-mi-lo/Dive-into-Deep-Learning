{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "918132b8-813e-4c7f-9416-0034b9aa2d50",
   "metadata": {},
   "source": [
    "### 6.2. Parameter Management\n",
    " \n",
    "Once we have chosen an architecture and set our hyperparameters, we proceed to the training loop, where our goal is to find parameter values that minimize our loss function. After training, we will need these parameters in order to make future predictions. Additionally, we will sometimes wish to extract the parameters perhaps to reuse them in some other context, to save our model to disk so that it may be executed in other software, or for examination in the hope of gaining scientific understanding.\n",
    "\n",
    "Most of the time, we will be able to ignore the nitty-gritty details of how parameters are declared and manipulated, relying on deep learning frameworks to do the heavy lifting. However, when we move away from stacked architectures with standard layers, we will sometimes need to get into the weeds of declaring and manipulating parameters. In this section, we cover the following:\n",
    "\n",
    "* Accessing parameters for debugging, diagnostics, and visualizations.\n",
    "\n",
    "* Sharing parameters across different model com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ea04372-29fe-4856-ba1f-b01e6434f4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34bfef0-7311-40cf-b7f0-951e6a1ecc31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2535fc28-f071-4633-afcd-610b7152652b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.reshaping.flatten.Flatten at 0x7fc9e19a2e90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x7fc9e19a2a10>,\n",
       " <keras.src.layers.core.dense.Dense at 0x7fc9e1102d40>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee14cfc6-88cd-4640-a25f-1d8f9fb8027c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer name: flatten_2, Output shape: (2, 4)\n",
      "Layer name: dense_4, Output shape: (2, 4)\n",
      "Layer name: dense_5, Output shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in net.layers:\n",
    "  print(f\"Layer name: {layer.name}, Output shape: {layer.output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05a9a301-21e4-43b5-9960-5a50a98c8b30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1b3076-3f51-4af1-ba87-fd6ad610d63c",
   "metadata": {},
   "source": [
    "#### 6.2.1. Parameter Access\n",
    "\n",
    "Let’s start with how to access parameters from the models that you already know.\n",
    "When a model is defined via the Sequential class, we can first access any layer by indexing into the model as though it were a list. Each layer’s parameters are conveniently located in its attribute.\n",
    "We can inspect the parameters of the second fully connected layer as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f00441-2e2c-4a14-a178-3c34bd31573c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(4, 1) dtype=float32, numpy=\n",
       " array([[-0.78668416],\n",
       "        [ 0.4327531 ],\n",
       "        [-0.40664268],\n",
       "        [-0.28720444]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[2].weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61567a4-a244-4c8b-b50d-05909d463950",
   "metadata": {},
   "source": [
    "We can see that this fully connected layer contains two parameters, corresponding to that layer’s weights and biases, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ebda-589d-40b3-8afa-994d03e53ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 6.2.1.1. Targeted Parameters\n",
    "\n",
    "Note that each parameter is represented as an instance of the parameter class. To do anything useful with the parameters, we first need to access the underlying numerical values. There are several ways to do this. Some are simpler while others are more general. The following code extracts the bias from the second neural network layer, which returns a parameter class instance, and further accesses that parameter’s value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a7f299-2e6d-4202-8c80-c55ef7c92b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensorflow.python.ops.resource_variable_ops.ResourceVariable,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.layers[2].weights[1]), tf.convert_to_tensor(net.layers[2].weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6697552-f627-4c90-b113-aa7b8b8397cb",
   "metadata": {},
   "source": [
    "#### 6.2.1.2. All Parameters at Once\n",
    "When we need to perform operations on all parameters, accessing them one-by-one can grow tedious. The situation can grow especially unwieldy when we work with more complex, e.g., nested, modules, since we would need to recurse through the entire tree to extract each sub-module’s parameters. Below we demonstrate accessing the parameters of all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f4a3c91-3021-4323-9f47-1789bf39b550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<keras.src.layers.reshaping.flatten.Flatten at 0x7fc9e19a2e90>,\n",
       "  <keras.src.layers.core.dense.Dense at 0x7fc9e19a2a10>,\n",
       "  <keras.src.layers.core.dense.Dense at 0x7fc9e1102d40>],\n",
       " [array([[ 0.03152835, -0.20600021, -0.11119342,  0.13039881],\n",
       "         [ 0.8349889 , -0.36418748, -0.58995926,  0.5881589 ],\n",
       "         [ 0.23226136,  0.49354047, -0.5157879 ,  0.10499209],\n",
       "         [-0.43608382, -0.23599237, -0.5114808 ,  0.36620098]],\n",
       "        dtype=float32),\n",
       "  array([0., 0., 0., 0.], dtype=float32),\n",
       "  array([[-0.23940843],\n",
       "         [ 0.7190784 ],\n",
       "         [ 0.5216007 ],\n",
       "         [-0.35595208]], dtype=float32),\n",
       "  array([0.], dtype=float32)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers,   net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9c82f-0815-4f01-80ea-0ce1d0261381",
   "metadata": {},
   "source": [
    "#### 6.2.2. Tied Parameters\n",
    "Often, we want to share parameters across multiple layers. Let’s see how to do this elegantly. In the following we allocate a fully connected layer and then use its parameters specifically to set those of another layer. Here we need to run the forward propagation net(X) before accessing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9209679-43cf-4fab-a066-7f1bd830b6e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# tf.keras behaves a bit differently. It removes the duplicate layer\n",
    "# automatically\n",
    "shared = tf.keras.layers.Dense(4, activation=tf.nn.relu)\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    shared,\n",
    "    shared,\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "# Check whether the parameters are different\n",
    "print(len(net.layers) == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc8ffc-9f94-4222-bd0e-b060b4e6729f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.3. Parameter Initialization\n",
    " \n",
    "Now that we know how to access the parameters, let’s look at how to initialize them properly. We discussed the need for proper initialization in Section 5.4. The deep learning framework provides default random initializations to its layers. However, we often want to initialize our weights according to various other protocols. The framework provides most commonly used protocols, and also allows to create a custom initializer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72710c3-07b6-426d-ae1b-bfcc35548aa0",
   "metadata": {},
   "source": [
    "By default, Keras initializes weight matrices uniformly by drawing from a range that is computed according to the input and output dimension, and the bias parameters are all set to zero. TensorFlow provides a variety of initialization methods both in the root module and the keras.initializers module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "830a648f-2b2f-4063-b9ab-71a62ec0dfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 4))\n",
    "net(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0bedf6-14ef-437c-ba55-e6016369afd8",
   "metadata": {},
   "source": [
    "#### 6.3.1. Built-in Initialization\n",
    "Let’s begin by calling on built-in initializers. The code below initializes all weight parameters as Gaussian random variables with standard deviation 0.01, while bias parameters are cleared to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b271f36a-73a8-4a03-8868-8ea229910c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_12/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[ 0.0025312 , -0.02097435,  0.01360615, -0.00122448],\n",
       "        [-0.02847254, -0.00691849,  0.01073523,  0.00374302],\n",
       "        [ 0.00736204,  0.00382908,  0.00891054, -0.01125475],\n",
       "        [ 0.00410018, -0.00641336, -0.0057174 , -0.00123267]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_12/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.01),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1)])\n",
    "\n",
    "net(X) # weights get created with the instanceiation. else error\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb05717-66de-49ce-9aab-e23a0981fc10",
   "metadata": {},
   "source": [
    "We can also initialize all the parameters to a given constant value (say, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9c71e4f-6070-428a-a43b-ca54194ea137",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'dense_14/kernel:0' shape=(4, 4) dtype=float32, numpy=\n",
       " array([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]], dtype=float32)>,\n",
       " <tf.Variable 'dense_14/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        4, activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.Constant(1),\n",
    "        bias_initializer=tf.zeros_initializer()),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "net.weights[0], net.weights[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d96d8-1a86-4ffb-ba7a-d5c252d127bd",
   "metadata": {},
   "source": [
    "We can also apply different initializers for certain blocks. For example, below we initialize the first layer with the Xavier initializer and initialize the second layer to a constant value of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1b6e3c09-46b3-406c-8170-f6b08db1fc69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_24/kernel:0' shape=(4, 5) dtype=float32, numpy=\n",
      "array([[-0.13218945,  0.75522876, -0.75359315,  0.15835404,  0.7877854 ],\n",
      "       [ 0.6005722 , -0.02939814,  0.5569992 , -0.7584791 ,  0.63693726],\n",
      "       [ 0.46216822, -0.6875799 ,  0.8145089 ,  0.5875597 , -0.31115454],\n",
      "       [-0.58594376, -0.29458034,  0.4479493 ,  0.32112324,  0.05051011]],\n",
      "      dtype=float32)>\n",
      "<tf.Variable 'dense_25/kernel:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[42.],\n",
      "       [42.],\n",
      "       [42.],\n",
      "       [42.],\n",
      "       [42.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(5 ,activation=tf.nn.relu,\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform()),\n",
    "    tf.keras.layers.Dense(\n",
    "        1, kernel_initializer=tf.keras.initializers.Constant(42)),\n",
    "])\n",
    "\n",
    "net(X)\n",
    "print(net.layers[1].weights[0])\n",
    "print(net.layers[2].weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed73fca-5837-4a15-9cd7-2ca645ed9a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
